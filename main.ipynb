{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: GPU Verification\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected - using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Core Setup\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Global Settings\n",
    "class Config:\n",
    "    # Data settings\n",
    "    expected_labels = ['angry', 'fear', 'happy', 'neutral', 'sad']\n",
    "    audio_max_duration = 3  # seconds\n",
    "    sample_rate = 16000\n",
    "    \n",
    "    # Model settings\n",
    "    model_name = \"facebook/wav2vec2-base\"\n",
    "    batch_size = 4\n",
    "    learning_rate = 3e-5\n",
    "    num_epochs = 30\n",
    "    \n",
    "    # Path handling\n",
    "    base_path = Path(\"dataset\")\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "label\n",
      "happy      134\n",
      "neutral    134\n",
      "sad        133\n",
      "angry      127\n",
      "fear        70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test dataset:\n",
      "label\n",
      "neutral    42\n",
      "happy      42\n",
      "sad        42\n",
      "angry      40\n",
      "fear       22\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading & Cleaning\n",
    "def load_and_validate_dataset(csv_path):\n",
    "    \"\"\"Load dataset with comprehensive validation\"\"\"\n",
    "    try:\n",
    "        # Detect header presence\n",
    "        with open(csv_path, 'r') as f:\n",
    "            first_line = f.readline().strip().lower()\n",
    "            has_header = any(label in first_line for label in ['path', 'audio', 'label', 'emotion'])\n",
    "        \n",
    "        df = pd.read_csv(\n",
    "            csv_path,\n",
    "            header=0 if has_header else None,\n",
    "            names=[\"audio_path\", \"label\"]\n",
    "        )\n",
    "        \n",
    "        # Clean paths\n",
    "        df[\"audio_path\"] = df[\"audio_path\"].apply(\n",
    "            lambda x: str(Path(x.replace(\"\\\\\", os.sep).replace(\"/\", os.sep)))\n",
    "        )\n",
    "        \n",
    "        # Clean labels\n",
    "        df[\"label\"] = df[\"label\"].str.strip().str.lower()\n",
    "        df[\"label\"] = df[\"label\"].replace({'emotion': 'neutral'})  # Fix observed error\n",
    "        \n",
    "        # Validate labels\n",
    "        invalid_labels = set(df[\"label\"]) - set(config.expected_labels)\n",
    "        if invalid_labels:\n",
    "            raise ValueError(f\"Invalid labels found: {invalid_labels}\")\n",
    "            \n",
    "        # Check file existence\n",
    "        missing_files = [p for p in df[\"audio_path\"] if not Path(p).exists()]\n",
    "        if missing_files:\n",
    "            raise FileNotFoundError(f\"Missing {len(missing_files)} audio files\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {csv_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_df = load_and_validate_dataset(\"train_dataset.csv\")\n",
    "    test_df = load_and_validate_dataset(\"test_dataset.csv\")\n",
    "    \n",
    "    print(\"Train dataset:\")\n",
    "    print(train_df[\"label\"].value_counts())\n",
    "    print(\"\\nTest dataset:\")\n",
    "    print(test_df[\"label\"].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed to load datasets:\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset Pipeline\n",
    "class TamilEmotionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_length = config.audio_max_duration * config.sample_rate\n",
    "        \n",
    "        # Create label map\n",
    "        self.label_map = {label: idx for idx, label in enumerate(config.expected_labels)}\n",
    "        self.inverse_map = {v: k for k, v in self.label_map.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load metadata\n",
    "            audio_path = self.df.iloc[idx][\"audio_path\"]\n",
    "            label = self.df.iloc[idx][\"label\"]\n",
    "            \n",
    "            # Validate label\n",
    "            if label not in self.label_map:\n",
    "                raise ValueError(f\"Invalid label {label}\")\n",
    "                \n",
    "            # Load audio\n",
    "            waveform, sr = librosa.load(\n",
    "                audio_path,\n",
    "                sr=config.sample_rate,\n",
    "                mono=True,\n",
    "                duration=config.audio_max_duration\n",
    "            )\n",
    "            \n",
    "            # Validate audio\n",
    "            if len(waveform) < 0.5 * sr:  # Minimum 0.5s\n",
    "                raise ValueError(\"Audio too short\")\n",
    "                \n",
    "            # Process features\n",
    "            inputs = self.processor(\n",
    "                waveform,\n",
    "                sampling_rate=sr,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"input_values\": inputs[\"input_values\"].squeeze(),\n",
    "                \"labels\": torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {audio_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Handle invalid samples\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    return {\n",
    "        \"input_values\": torch.stack([b[\"input_values\"] for b in batch]),\n",
    "        \"labels\": torch.stack([b[\"labels\"] for b in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\tamil_ser\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 (Revised): Model Setup with Explicit GPU Handling\n",
    "try:\n",
    "    # Initialize processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(config.model_name)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TamilEmotionDataset(train_df, processor)\n",
    "    test_dataset = TamilEmotionDataset(test_df, processor)\n",
    "    \n",
    "    # Model config\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=len(config.expected_labels)\n",
    "    ).to(device)  # Explicit device placement\n",
    "    \n",
    "    print(\"\\nModel device:\", next(model.parameters()).device)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Model initialization failed:\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\tamil_ser\\venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ser_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=config.learning_rate,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,  # Keep only the last 3 checkpoints\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU available\n",
    "    report_to=\"none\",  # Disable external logging\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 23:13, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.536700</td>\n",
       "      <td>1.473981</td>\n",
       "      <td>0.324468</td>\n",
       "      <td>0.288825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.427000</td>\n",
       "      <td>1.357962</td>\n",
       "      <td>0.393617</td>\n",
       "      <td>0.320480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.307500</td>\n",
       "      <td>1.329978</td>\n",
       "      <td>0.393617</td>\n",
       "      <td>0.311017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.306628</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>0.378345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.185100</td>\n",
       "      <td>1.187783</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.479323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.932400</td>\n",
       "      <td>1.246325</td>\n",
       "      <td>0.515957</td>\n",
       "      <td>0.496519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>1.286096</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.492254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>1.113290</td>\n",
       "      <td>0.627660</td>\n",
       "      <td>0.612046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.573700</td>\n",
       "      <td>1.174123</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>0.598897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>1.448538</td>\n",
       "      <td>0.611702</td>\n",
       "      <td>0.611946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>1.910429</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.554488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>1.618786</td>\n",
       "      <td>0.686170</td>\n",
       "      <td>0.684468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>1.910994</td>\n",
       "      <td>0.664894</td>\n",
       "      <td>0.663746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>2.033312</td>\n",
       "      <td>0.654255</td>\n",
       "      <td>0.644731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>2.091860</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.635760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>2.107810</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.678619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>2.256727</td>\n",
       "      <td>0.675532</td>\n",
       "      <td>0.665291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>2.241788</td>\n",
       "      <td>0.686170</td>\n",
       "      <td>0.677697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>2.175826</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.687586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>2.344721</td>\n",
       "      <td>0.675532</td>\n",
       "      <td>0.668013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>2.178535</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.707764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>2.223890</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.698700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>2.322067</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.677782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>2.071661</td>\n",
       "      <td>0.718085</td>\n",
       "      <td>0.717319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>2.429052</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.677585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>2.265814</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0.711264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>2.530607</td>\n",
       "      <td>0.686170</td>\n",
       "      <td>0.685160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.653224</td>\n",
       "      <td>0.675532</td>\n",
       "      <td>0.674085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.654300</td>\n",
       "      <td>0.675532</td>\n",
       "      <td>0.674085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.668643</td>\n",
       "      <td>0.675532</td>\n",
       "      <td>0.674085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Final metrics: {'train_runtime': 1395.899, 'train_samples_per_second': 12.852, 'train_steps_per_second': 3.224, 'total_flos': 4.8861599222592e+17, 'train_loss': 0.3950441453854243, 'epoch': 30.0}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Start Training\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Final metrics: {train_result.metrics}\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"CUDA out of memory\" in str(e):\n",
    "        print(\"Memory error! Reduce batch size or model size\")\n",
    "    else:\n",
    "        print(\"Training failed:\")\n",
    "    raise\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Unexpected error during training:\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best checkpoint at: ser_results\\checkpoint-3600\n",
      "Model loaded successfully!\n",
      "\n",
      "Predicted emotion: happy\n",
      "Confidence: 99.95%\n",
      "\n",
      "All probabilities:\n",
      "angry: 0.02%\n",
      "fear: 0.01%\n",
      "happy: 99.95%\n",
      "neutral: 0.01%\n",
      "sad: 0.02%\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Model Loading and Inference\n",
    "def load_trained_model(model_path):\n",
    "    \"\"\"Load the trained model from checkpoint\"\"\"\n",
    "    try:\n",
    "        # Load model with classification head\n",
    "        model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            model_path,\n",
    "            num_labels=len(config.expected_labels)\n",
    "        ).to(device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def find_best_checkpoint():\n",
    "    \"\"\"Find the best checkpoint based on evaluation metrics\"\"\"\n",
    "    try:\n",
    "        results_dir = Path(\"./ser_results\")\n",
    "        checkpoints = [d for d in results_dir.glob(\"checkpoint-*\") if d.is_dir()]\n",
    "        \n",
    "        if not checkpoints:\n",
    "            return None\n",
    "            \n",
    "        # Look for trainer_state.json in each checkpoint\n",
    "        best_score = -float('inf')\n",
    "        best_checkpoint = None\n",
    "        \n",
    "        for checkpoint in checkpoints:\n",
    "            state_file = checkpoint / \"trainer_state.json\"\n",
    "            if state_file.exists():\n",
    "                with open(state_file, 'r') as f:\n",
    "                    import json\n",
    "                    state = json.load(f)\n",
    "                    # Get the best metric score\n",
    "                    if 'best_metric' in state and state['best_metric'] > best_score:\n",
    "                        best_score = state['best_metric']\n",
    "                        best_checkpoint = checkpoint\n",
    "        \n",
    "        return best_checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding best checkpoint: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Find and load the best checkpoint\n",
    "best_checkpoint = find_best_checkpoint()\n",
    "if best_checkpoint:\n",
    "    print(f\"Found best checkpoint at: {best_checkpoint}\")\n",
    "    model = load_trained_model(str(best_checkpoint))\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No checkpoints found! Please train the model first.\")\n",
    "\n",
    "def predict_emotion(audio_path):\n",
    "    try:\n",
    "        # Load and preprocess audio\n",
    "        waveform, sr = librosa.load(\n",
    "            audio_path,\n",
    "            sr=config.sample_rate,\n",
    "            mono=True,\n",
    "            duration=config.audio_max_duration\n",
    "        )\n",
    "        \n",
    "        # Process through wav2vec processor\n",
    "        inputs = processor(\n",
    "            waveform,\n",
    "            sampling_rate=sr,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config.audio_max_duration * config.sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Move input to same device as model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_id = torch.argmax(predictions, dim=-1).item()\n",
    "            confidence = predictions[0][predicted_id].item()\n",
    "        \n",
    "        # Map prediction to emotion label\n",
    "        predicted_emotion = config.expected_labels[predicted_id]\n",
    "        \n",
    "        return {\n",
    "            'emotion': predicted_emotion,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': {\n",
    "                label: prob.item()\n",
    "                for label, prob in zip(config.expected_labels, predictions[0])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting emotion: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage with proper error handling:\n",
    "test_audio = \"01_04_sad.wav\"  \n",
    "if best_checkpoint is None:\n",
    "    print(\"Error: No trained model checkpoint found! Please train the model first!\")\n",
    "elif not os.path.exists(test_audio):\n",
    "    print(\"Error: Please provide a valid audio file path\")\n",
    "else:\n",
    "    try:\n",
    "        result = predict_emotion(test_audio)\n",
    "        print(f\"\\nPredicted emotion: {result['emotion']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "        print(\"\\nAll probabilities:\")\n",
    "        for emotion, prob in result['probabilities'].items():\n",
    "            print(f\"{emotion}: {prob:.2%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
